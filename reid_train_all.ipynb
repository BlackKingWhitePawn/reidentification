{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Modules. Only for colab running_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pathlib\n",
    "\n",
    "# debug\n",
    "DEBUG = False\n",
    "\n",
    "# enviroment\n",
    "DATA_PATH = join(pathlib.Path().resolve(), 'data')\n",
    "\n",
    "# constants\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# data statistics\n",
    "MOT20_EXT_FIRST_AXIS_MEAN = 139\n",
    "MOT20_EXT_SECOND_AXIS_MEAN = 62\n",
    "MOT20_EXT_MEAN = None\n",
    "MOT20_EXT_STD = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOT20_ext data format:\n",
    "train:\n",
    "|\n",
    "|- <video_id> // директория, содержащая объекты, вырезанные из соответствующего видео\n",
    "    |\n",
    "    |- det\n",
    "    |   |\n",
    "    |   |- det.txt // файл описания детекций, хранящий строки в формате <frame number>, <object id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <confidence>, <x>, <y>, <z>\n",
    "    |\n",
    "    |- gt\n",
    "    |   |\n",
    "    |   |- gt.txt // файл описания ground truth, хранящий строки в формате <frame number>, <object id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <is_consider>, <class>, <visibility>\n",
    "    |\n",
    "    |- <object_id> // директория, содержащая изображения вырезанных объектов и файл описания\n",
    "        |\n",
    "        |a\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        |- <frame_id>.jpg - вырезанная из кадра frame_id область с объектом object_id\n",
    "\"\"\"\n",
    "\n",
    "from os import listdir, mkdir\n",
    "from os.path import exists, isdir, join\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "DET_COLUMNS = ['frame', 'id', 'bb_left', 'bb_top',\n",
    "               'bb_width', 'bb_height', 'confidence', 'x', 'y', 'z']\n",
    "DET_TYPES = {\n",
    "    'frame': int,\n",
    "    'id': int,\n",
    "    'bb_left': int,\n",
    "    'bb_top': int,\n",
    "    'bb_width': int,\n",
    "    'bb_height': int,\n",
    "    'confidence': int,\n",
    "    'x': int,\n",
    "    'y': int,\n",
    "    'z': int\n",
    "}\n",
    "\n",
    "GT_COLUMNS = ['frame', 'id', 'bb_left', 'bb_top',\n",
    "              'bb_width', 'bb_height', 'is_consider', 'class', 'visibility']\n",
    "\n",
    "GT_TYPES = {\n",
    "    'frame': int,\n",
    "    'id': int,\n",
    "    'bb_left': int,\n",
    "    'bb_top': int,\n",
    "    'bb_width': int,\n",
    "    'bb_height': int,\n",
    "    'is_consider': int,\n",
    "    'class': int,\n",
    "    'visibility': float\n",
    "}\n",
    "\n",
    "\n",
    "def __get_file_name_by_id(frame_id: int) -> str:\n",
    "    return f'{str.zfill(str(frame_id), 6)}.jpg'\n",
    "\n",
    "\n",
    "def __extract_objects_from_frame(video_path: str, frame: str, data: pd.DataFrame) -> None:\n",
    "    \"\"\"Извлекает все объекты из кадра. Сохраняет список вырезанных объектов в соответствующие директории\n",
    "    ### Parameters:\n",
    "    - video_path: str - путь до директории с текущим видео\n",
    "    - frame: str - путь до изображения, из которого вырезаются объекты\n",
    "    - data: pandas.DataFrame - датафрейм с данными объектов на данном изображении\n",
    "    \"\"\"\n",
    "    current_image = Image.open(frame)\n",
    "    objects_to_crop = data.to_dict('records')\n",
    "    for obj in objects_to_crop:\n",
    "        x, y, w, h = obj['bb_left'], obj['bb_top'], obj['bb_width'], obj['bb_height']\n",
    "        img = current_image.crop(box=(x, y, x + w, y + h))\n",
    "        img.save(join(video_path, str(\n",
    "            int(obj['id'])), __get_file_name_by_id(obj['frame'])))\n",
    "\n",
    "\n",
    "def __extract_video(mot20_video_path: str, mot20_video_ext_path: str) -> None:\n",
    "    \"\"\"Извлекает все объекты из видео\n",
    "    ### Parameters: \n",
    "    - mot20_video_path: str - путь до директории с видео в датасете MOT20\n",
    "    - mot20_video_ext_path: str - путь до директории с видео в датасете MOT20_ext\n",
    "    \"\"\"\n",
    "    detections = get_dataframe(mot20_video_path, file_type='det')\n",
    "    ground_truth = get_dataframe(mot20_video_path, file_type='gt')\n",
    "    # выбираем объекты, которые стоит рассматривать и которые относятся к классу пешеходов или стоящих людей\n",
    "    persons = ground_truth[((ground_truth['class'] == 1) | (\n",
    "        ground_truth['class'] == 7)) & ground_truth['is_consider'] == 1]\n",
    "    # для каждого объекта создаем директорию\n",
    "    for id in persons['id'].unique():\n",
    "        mkdir(join(mot20_video_ext_path, str(id)))\n",
    "    # проходим по всем кадрам видео\n",
    "    for frame in tqdm(persons['frame'].unique()):\n",
    "        __extract_objects_from_frame(\n",
    "            mot20_video_ext_path,\n",
    "            join(\n",
    "                mot20_video_path, 'img1', f'{str.zfill(str(frame), 6)}.jpg'),\n",
    "            persons[persons['frame'] == frame]\n",
    "        )\n",
    "        # save_objects(mot20_video_ext_path, objects)\n",
    "\n",
    "\n",
    "def get_dataframe(video_path: str, file_type: str = 'det') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Возвращает датафрейм с обнаружениями или ground truth для указанного видео\n",
    "    ### Parameters\n",
    "    - video_path: str - путь до директории с видео\n",
    "    - file_type: str - det для файла с обнаружениями, gt для ground truth\n",
    "    \"\"\"\n",
    "    df = None\n",
    "\n",
    "    if (file_type == 'det'):\n",
    "        df = pd.read_csv(\n",
    "            join(video_path, 'det', 'det.txt'),\n",
    "            names=DET_COLUMNS,\n",
    "            dtype=DET_TYPES\n",
    "        )\n",
    "    else:\n",
    "        df = pd.read_csv(\n",
    "            join(video_path, 'gt', 'gt.txt'),\n",
    "            names=GT_COLUMNS,\n",
    "            dtype=GT_TYPES\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def run(data_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Выполняет преобразование датасета MOT20\n",
    "    ### Parameters\n",
    "    - data_path: str - путь до директории с датасетами\n",
    "    \"\"\"\n",
    "    mot20_ext_path = join(data_path, 'MOT20_ext')\n",
    "    mot20_path = join(data_path, 'MOT20')\n",
    "    # создание директорий\n",
    "    if (not (exists(mot20_ext_path) and isdir(mot20_ext_path))):\n",
    "        # создаем основную директорию\n",
    "        mkdir(mot20_ext_path)\n",
    "        # мы используем данные для трейна, так как мы не будем обучаться на тесте\n",
    "        mkdir(join(mot20_ext_path, 'train'))\n",
    "        # проходим по всем видео в исходной\n",
    "        for video_id in listdir(join(mot20_path, 'train')):\n",
    "            # сохраняем пути до видео в исходной и в новой директориях\n",
    "            current_path = join(\n",
    "                join(data_path, 'MOT20'), 'train', video_id)\n",
    "            current_path_ext = join(mot20_ext_path, 'train', video_id)\n",
    "            mkdir(current_path_ext)\n",
    "            __extract_video(current_path, current_path_ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "\n",
    "def __aggregate_to_continious(x: int | list[list[int]], y: int):\n",
    "    \"\"\"Аггрегирующая функция для разделения списка чисел на непрерывные отрезки\"\"\"\n",
    "    if (not type(x) == list):\n",
    "        if (y - x == 1):\n",
    "            return [[x, y]]\n",
    "        else:\n",
    "            return [[x], [y]]\n",
    "    else:\n",
    "        last = x[-1][-1]\n",
    "        if (y - last > 1):\n",
    "            x.append([y])\n",
    "        else:\n",
    "            x[-1].append(y)\n",
    "    return x\n",
    "\n",
    "\n",
    "def __get_possible_tuples_count_segment(distance: int, segment: list[int]) -> int:\n",
    "    \"\"\"Рассчитывает количество возможных пар для отрезка\"\"\"\n",
    "    return len(segment) - distance - 1\n",
    "\n",
    "\n",
    "def __get_neighbours_tuples_count(distance: int, segments: list[list[int]]) -> int:\n",
    "    \"\"\"Рассчитывает количество возможных пар из граничных элементов\"\"\"\n",
    "    sum = 0\n",
    "    prev = None\n",
    "    for s in segments:\n",
    "        if (prev == None):\n",
    "            prev = s\n",
    "            continue\n",
    "        if (s[0] - prev[-1] - 1 == distance):\n",
    "            sum += 1\n",
    "        prev = s\n",
    "\n",
    "    return sum\n",
    "\n",
    "\n",
    "def get_possible_tuples_count(distance: int, segments: list[list[int]]) -> int:\n",
    "    \"\"\"Рассчитывает количество возможных пар для списка отрезков\"\"\"\n",
    "    sum = 0\n",
    "    for s in segments:\n",
    "        sum += max(0, __get_possible_tuples_count_segment(distance, s))\n",
    "\n",
    "    sum += __get_neighbours_tuples_count(distance, segments)\n",
    "    return sum\n",
    "\n",
    "\n",
    "def split_to_continuous_segments(array_numbers: list[int]) -> list[list[int]]:\n",
    "    \"\"\"Возвращает список непрерывных отрезков чисел\"\"\"\n",
    "    if (len(array_numbers) == 0):\n",
    "        return [[]]\n",
    "    elif (len(array_numbers) == 1):\n",
    "        return [array_numbers]\n",
    "    else:\n",
    "        return functools.reduce(__aggregate_to_continious, sorted(array_numbers))\n",
    "\n",
    "\n",
    "def __get_possible_tuples(distance: int, segment: list[int]) -> list[tuple[int, int]]:\n",
    "    end = max(len(segment) - distance - 1, 0)\n",
    "    return [(i, i + distance + 1) for i in range(segment[0], segment[end])]\n",
    "\n",
    "\n",
    "def get_possible_tuples(distance: int, segments: list[list[int]]) -> list[tuple[int, int]]:\n",
    "    \"\"\"Возвращает список возможных пар чисел с заданным расстоянием для отрезка\n",
    "    ### Parameters: \n",
    "    - distance: int - расстояние между элементами\n",
    "    - segments: list[list[int]] - список непрерывных отрезков\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    prev = None\n",
    "    for segment in segments:\n",
    "        tuples = __get_possible_tuples(distance, segment)\n",
    "        if (prev is not None and segment[0] - prev[-1] - 1 == distance):\n",
    "            res.append((prev[-1], segment[0]))\n",
    "\n",
    "        prev = segment\n",
    "        res += tuples\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Содержит классы для загрузки данных MOT20_ext, преобразованных из MOT20 dataset\n",
    "MOT20_ext data format:\n",
    "train:\n",
    "|\n",
    "|- <video_id> // директория, содержащая объекты, вырезанные из соответствующего видео\n",
    "    |\n",
    "    |- det\n",
    "    |   |\n",
    "    |   |- det.txt // файл описания детекций, хранящий строки в формате <frame number>, <object id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <confidence>, <x>, <y>, <z>\n",
    "    |\n",
    "    |- gt\n",
    "    |   |\n",
    "    |   |- gt.txt // файл описания ground truth, хранящий строки в формате <frame number>, <object id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <is_consider>, <class>, <visibility>\n",
    "    |\n",
    "    |- <object_id> // директория, содержащая изображения вырезанных объектов и файл описания\n",
    "        |\n",
    "        |\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        |- <frame_id>.jpg - вырезанная из кадра frame_id область с объектом object_id\n",
    "\"\"\"\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "import cv2\n",
    "from numpy.random import choice\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "\n",
    "class MOT20ExtDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Создает объект типа Dataset, загружающий данные преобразованного датасета MOT20_ext\n",
    "    Возвращает пары изображений и метку: 1, если на изображении один и тот же объект, иначе 0 \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "            video_path: str,\n",
    "            transform=None,\n",
    "            visibility_threshold: float = 1,\n",
    "            frame_distance: int | list[int] | tuple[int, int] = 0,\n",
    "            negative_proportion: float = 0.5\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Создает объект типа Dataset, загружающий данные преобразованного датасета MOT20_ext.\n",
    "        Возвращает пары изображений и метку: 0, если на изображении один и тот же объект, иначе 1 \n",
    "        ### Parameters:\n",
    "        - video_path: str - путь до директории с видео датасета МОТ20_ехт. Ожидается, что в директории находятся файлы описаний и ground truth\n",
    "        - transform - применяемые аугментации\n",
    "        - visibility_threshold: float - порог видимости (поле visibility) объекта, используемого в обучении\n",
    "        - frame_distance: int | list[int] | tuple[int, int] - допустимое расстояние между кадрами, объекты из которых используются в обучении. Если переданы два числа в виде начального и конечного значений - конечное включается \n",
    "        - negative_proportion: float - доля объектов, значение метки для которых 0\n",
    "        \"\"\"\n",
    "        super(MOT20ExtDataset).__init__()\n",
    "        self.video_path = video_path\n",
    "        self.visibility_threshold = visibility_threshold\n",
    "        self._check_distance_correct(frame_distance)\n",
    "        self.frame_distance = frame_distance\n",
    "        self.detections = get_dataframe(video_path, file_type='det')\n",
    "        df = get_dataframe(video_path, file_type='gt')\n",
    "        # берем объекты которые стоит учитывать при обучении\n",
    "        df = df[df['is_consider'] == 1]\n",
    "        # выбираем с видимостью выше заданной\n",
    "        df = df[df['visibility'] >= visibility_threshold]\n",
    "        self.ground_truth = df\n",
    "        # формируем словарь, используемый для длины и индексации\n",
    "        self._objetcs_pairs_dict = self._get_pairs_dict()\n",
    "        # рассчитываем количество объектов с меткой 1\n",
    "        self._len_1 = self._calc_len()\n",
    "        # рассчитываем длину всего датасета\n",
    "        self._len = round(self._len_1 / (1 - negative_proportion))\n",
    "        self.transform = transform\n",
    "\n",
    "    def _check_distance_correct(self, distance: int) -> None:\n",
    "        \"\"\"Проверяет корректность типов и значений для расстояния\"\"\"\n",
    "        if (type(distance) == int):\n",
    "            if (distance < 0):\n",
    "                raise ValueError(\n",
    "                    'Distance between frames must be non negative integer')\n",
    "        elif (type(distance) == list):\n",
    "            for d in distance:\n",
    "                if (not type(d) == int):\n",
    "                    raise TypeError(\n",
    "                        'Each distance value must be non negative integer')\n",
    "                if (distance < 0):\n",
    "                    raise ValueError(\n",
    "                        'Distance between frames must be non negative integer')\n",
    "        elif (type(distance) == tuple):\n",
    "            start, end = distance\n",
    "            if (not (type(start) == int and type(end) == int)):\n",
    "                raise TypeError(\n",
    "                    'Each distance value must be non negative integer')\n",
    "            if (start < 1):\n",
    "                raise ValueError(\n",
    "                    'Start index of distance must be non negative')\n",
    "            if (end < start):\n",
    "                raise ValueError(\n",
    "                    'End index of distance must be bigger then start')\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                'Distance argument must be integre or list of integres or tuple of two integres')\n",
    "\n",
    "    def _get_pairs_dict(self) -> dict[int, int | dict[int, int]]:\n",
    "        \"\"\"Возвращает словарь, содержащий количество возможных пар для каждого объекта. В случае нескольких d набор пар представлен как словари\"\"\"\n",
    "        objects_lens = {}\n",
    "        for object_id in sorted(self.ground_truth['id'].unique()):\n",
    "            object_frames = sorted(\n",
    "                self.ground_truth[self.ground_truth['id'] == object_id]['frame'].values)\n",
    "            segments = split_to_continuous_segments(object_frames)\n",
    "            if (type(self.frame_distance) == int):\n",
    "                objects_lens[object_id] = get_possible_tuples_count(\n",
    "                    self.frame_distance, segments)\n",
    "            elif (type(self.frame_distance) == list):\n",
    "                objects_lens[object_id] = {}\n",
    "                for d in self.frame_distance:\n",
    "                    objects_lens[object_id][d] = get_possible_tuples_count(\n",
    "                        d, segments)\n",
    "            else:\n",
    "                start_d, end_d = self.frame_distance\n",
    "                objects_lens[object_id] = {}\n",
    "                for d in range(start_d, end_d + 1):\n",
    "                    objects_lens[object_id][d] = get_possible_tuples_count(\n",
    "                        d, segments)\n",
    "\n",
    "        return objects_lens\n",
    "\n",
    "    def _calc_len(self) -> int:\n",
    "        \"\"\"Рассчитывает длину датасета\"\"\"\n",
    "        count = 0\n",
    "        for v in self._objetcs_pairs_dict.values():\n",
    "            if (type(v) == int):\n",
    "                count += v\n",
    "            else:\n",
    "                count += sum(v.values())\n",
    "\n",
    "        return count\n",
    "\n",
    "    def _get_pairs_by_idx(self, idx: int) -> tuple[int, int, int, int]:\n",
    "        \"\"\"Возвращает пару объектов класса 1, распологающуюся в датасете по индексу idx,\n",
    "        в формате tuple - (id первого объекта, id второго объекта, frame_id первого объекта, frame_id второго объекта)\"\"\"\n",
    "        previous_pairs_count = 0\n",
    "        object_id = -1\n",
    "        object_pairs = None\n",
    "        # поиск по парам, объекты на которых совпадают\n",
    "        # поиск по всем парам всех объектов со всеми возможными дистанциями\n",
    "        for id, pairs in self._objetcs_pairs_dict.items():\n",
    "            # считаем, сколько пар есть у данного объекта\n",
    "            if (type(pairs) == int):\n",
    "                previous_pairs_count += pairs\n",
    "            else:\n",
    "                previous_pairs_count = sum(pairs.values())\n",
    "\n",
    "            # если индекс больше, чем пар у текущего объекта - берем следующий\n",
    "            if (idx >= previous_pairs_count):\n",
    "                continue\n",
    "            else:\n",
    "                #  ищем среди пар данного объекта\n",
    "                iidx = idx - previous_pairs_count\n",
    "                object_frames = sorted(\n",
    "                    self.ground_truth[self.ground_truth['id'] == id]['frame'].values)\n",
    "                segments = split_to_continuous_segments(object_frames)\n",
    "                object_id = id\n",
    "                if (type(pairs) == int):\n",
    "                    object_pairs = get_possible_tuples(\n",
    "                        self.frame_distance, segments)[iidx]\n",
    "                    return (object_id, object_id, *object_pairs)\n",
    "                else:\n",
    "                    # ищем необходимую дистанцию\n",
    "                    current_previous_pairs_count = 0\n",
    "                    for d, d_pairs_len in pairs:\n",
    "                        current_previous_pairs_count += d_pairs_len\n",
    "                        if (iidx >= current_previous_pairs_count):\n",
    "                            continue\n",
    "                        else:\n",
    "                            # ищем для пар с данной дистанцией\n",
    "                            iiidx = iidx - current_previous_pairs_count\n",
    "                            object_pairs = get_possible_tuples(d, segments)[\n",
    "                                iiidx]\n",
    "                            return (object_id, object_id, *object_pairs)\n",
    "\n",
    "    def _get_pairs0_by_idx(self, idx: int) -> tuple[int, int, int, int]:\n",
    "        \"\"\"Возвращает пару объектов класса 0, распологающуюся в датасете по индексу idx,\n",
    "        в формате tuple - (id первого объекта, id второго объекта, frame_id первого объекта, frame_id второго объекта).\n",
    "        Объект берется рандомно\n",
    "        \"\"\"\n",
    "        object_ids = self.ground_truth['id'].unique()\n",
    "        id1 = choice(object_ids)\n",
    "        frame_ids = self.ground_truth[self.ground_truth['id']\n",
    "                                      == id1]['frame']\n",
    "        frame1 = choice(frame_ids)\n",
    "        id2 = choice(object_ids)\n",
    "        frame_ids = self.ground_truth[self.ground_truth['id']\n",
    "                                      == id2]['frame']\n",
    "        frame2 = choice(frame_ids)\n",
    "\n",
    "        return (id1, id2, frame1, frame2)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[cv2.Mat, cv2.Mat, int]:\n",
    "        \"\"\"Возвращает два изображения в формате cv2.Mat и метку: 1, если на изображении один и тот же объект, иначе 0\n",
    "        Нумерация начинается с 0. Объекты в датасете хранятся в порядке:\n",
    "        - пары <объект;объект>\n",
    "            - пары <объект;объект> отсортированы по возрастанию id\n",
    "            - пары для одного объекта отсортированы по возрастанию distance, затем frame_id первого кадра пары\n",
    "        - пары <объект;другой_объект>\n",
    "            - рандомный объект\n",
    "        \"\"\"\n",
    "        id1, id2, frame_id1, frame_id2 = self._get_pairs_by_idx(\n",
    "            idx) if (idx < self._len_1) else self._get_pairs0_by_idx(idx)\n",
    "        img1 = cv2.imread(join(self.video_path, str(\n",
    "            id1), f'{str(frame_id1).zfill(6)}.jpg'))\n",
    "        img2 = cv2.imread(join(self.video_path, str(\n",
    "            id2), f'{str(frame_id2).zfill(6)}.jpg'))\n",
    "        if (self.transform):\n",
    "            img1 = self.transform(image=img1)['image']\n",
    "            img2 = self.transform(image=img2)['image']\n",
    "\n",
    "        return (img1, img2, 0 if (id1 == id2) else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_resize_transform(size: tuple[int, int]) -> A.Sequential:\n",
    "    \"\"\"Возвращает преобразование изменения размера\"\"\"\n",
    "    return A.Sequential([\n",
    "        A.Resize(*size),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_norm_transform(mean: list[int] = IMAGENET_MEAN, std: list[int] = IMAGENET_STD) -> A.Sequential:\n",
    "    \"\"\"Возвращает преобразование нормализации и приведения к тензору\"\"\"\n",
    "    return A.Sequential([\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from os.path import join\n",
    "\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Generator, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import ConcatDataset, DataLoader, random_split\n",
    "from torchvision import models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение сиамской сети для различия двух объектов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение на данных MOT20Ext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_transform = get_resize_transform((MOT20_EXT_FIRST_AXIS_MEAN, MOT20_EXT_SECOND_AXIS_MEAN)) \n",
    "norm_transform = get_norm_transform()\n",
    "transform = A.Compose([resize_transform, norm_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset01 = MOT20ExtDataset(join(DATA_PATH, 'MOT20_ext/train/MOT20-01/'), transform=transform)\n",
    "dataset02 = MOT20ExtDataset(join(DATA_PATH, 'MOT20_ext/train/MOT20-02/'), transform=transform)\n",
    "dataset03 = MOT20ExtDataset(join(DATA_PATH, 'MOT20_ext/train/MOT20-03/'), transform=transform)\n",
    "dataset05 = MOT20ExtDataset(join(DATA_PATH, 'MOT20_ext/train/MOT20-05/'), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ConcatDataset([dataset01, dataset02, dataset03, dataset05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271442"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание даталоадеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_PROPROTION = 0.2\n",
    "VAL_PROPORTION = 0.15\n",
    "TRAIN_PROPORTION = 1 - TEST_PROPROTION - VAL_PROPORTION\n",
    "sum([TEST_PROPROTION, VAL_PROPORTION, TRAIN_PROPORTION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = torch.manual_seed(0)\n",
    "dataset_use, dataset_unuse = random_split(dataset, [0.002, 0.998])\n",
    "len(dataset_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = random_split(\n",
    "    dataset_use, [TRAIN_PROPORTION, VAL_PROPORTION, TEST_PROPROTION], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    generator=generator,\n",
    "    \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    generator=generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 10, 13)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчет статистик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    generator=generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_statistics(loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка отображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_loader))\n",
    "# x1, x2, y = batch[0][0], batch[1][0], batch[2][0]\n",
    "# display_images((x1, x2), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_loader))\n",
    "# display_batch(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG - Delete after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/reidentification/.conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nick/reidentification/.conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBasicCNN(nn.Module):\n",
    "    \"\"\"Простейшая сиамская сверточная нейронная сеть\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(SiameseBasicCNN, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        # разморозим последний слой\n",
    "        for x in resnet.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in resnet.fc.parameters():\n",
    "            x.requires_grad = True\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        output1 = self.resnet(x1)\n",
    "        output2 = self.resnet(x2)\n",
    "\n",
    "        return F.pairwise_distance(\n",
    "            output1, output2, keepdim=True)\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"Функция потерь для двух объектов. Вычисляет евклидово расстояние между объектами\"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "# TODO: поменять\n",
    "    def forward(self, x, y):\n",
    "        loss_contrastive = torch.mean(\n",
    "            (1-y) * torch.pow(x, 2) +\n",
    "            y * torch.pow(torch.clamp(self.margin - x, min=0.0), 2))\n",
    "\n",
    "        return loss_contrastive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseBasicCNN2(nn.Module):\n",
    "    \"\"\"Простейшая сиамская сверточная нейронная сеть\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(SiameseBasicCNN2, self).__init__()\n",
    "        self.resnet = resnet\n",
    "        # разморозим последний слой\n",
    "        for x in resnet.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in resnet.layer4.parameters():\n",
    "            x.requires_grad = True\n",
    "        for x in resnet.fc.parameters():\n",
    "            x.requires_grad = True\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        output1 = self.resnet(x1)\n",
    "        output2 = self.resnet(x2)\n",
    "\n",
    "        return F.pairwise_distance(\n",
    "            output1, output2, keepdim=True)\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"Функция потерь для двух объектов. Вычисляет евклидово расстояние между объектами\"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "# TODO: поменять\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        loss_contrastive = torch.mean(\n",
    "            (1-y) * torch.pow(x, 2) +\n",
    "            y * torch.pow(torch.clamp(self.margin - x, min=0.0), 2))\n",
    "\n",
    "        return loss_contrastive\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: torch.nn.Module = None,\n",
    "    train_loader: DataLoader = None,\n",
    "    val_loader: DataLoader = None,\n",
    "    optimizer: Optimizer = None,\n",
    "    criterion = None,\n",
    "    epoch_count: int = 10,\n",
    "    scheduler: None = None,\n",
    "    threshold: float = 0.5,\n",
    "    device: torch.device = torch.device('cpu'),\n",
    "):\n",
    "    losses_train = []\n",
    "    accuracies_train = []\n",
    "    losses_val = []\n",
    "    accuracies_val = []\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        print('Epoch {}/{}:'.format(epoch, epoch_count - 1), flush=True)\n",
    "        for phase in ['train', 'val']:\n",
    "            if (phase == 'train'):\n",
    "                dataloader = train_loader\n",
    "                if (scheduler is not None):\n",
    "                    scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                dataloader = val_loader\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            # TODO: определить какая будет метрика качества\n",
    "\n",
    "            for (x1, x2, y) in tqdm(dataloader):\n",
    "                x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    distance = model(x1, x2)\n",
    "                    loss = criterion(distance, y)\n",
    "                    d = distance.clone()\n",
    "                    d[d <= threshold] = 0\n",
    "                    d[d > threshold] = 1\n",
    "                    \n",
    "                    if (phase == 'train'):\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_acc += torch.eq(d, y).float().mean()\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_acc = running_acc / len(dataloader)\n",
    "            if phase == 'val':\n",
    "                losses_val.append(epoch_loss)\n",
    "                accuracies_val.append(epoch_acc)\n",
    "            else:\n",
    "                losses_train.append(epoch_loss)\n",
    "                accuracies_train.append(epoch_acc)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc), flush=True)\n",
    "            \n",
    "            if phase == 'val' and best_val_accuracy < epoch_acc:\n",
    "                best_val_accuracy = epoch_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': epoch_loss,\n",
    "                    'model_name': model.name,\n",
    "                }, f'./{model.name}_{datetime.now().strftime(\"%d.%m_%H:%M\")}.pth')  # checkpoint_name + '_iou_{:.2f}_epoch_{}.pth'.format(self._max_score, i))\n",
    "                print(f'Model saved at {model.name}.pth')\n",
    "    \n",
    "    return model, {\n",
    "        'train': (losses_train, accuracies_train),\n",
    "        'val': (losses_val, accuracies_val)\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Настройка параметров обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 8/44 [00:03<00:16,  2.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(model\u001b[39m.\u001b[39mparameters(), lr)\n\u001b[1;32m      5\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model, results \u001b[39m=\u001b[39m train(\n\u001b[1;32m      8\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      9\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m     10\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     11\u001b[0m     train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m     12\u001b[0m     val_loader\u001b[39m=\u001b[39;49mval_loader,\n\u001b[1;32m     13\u001b[0m     epoch_count\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     15\u001b[0m )\n",
      "Cell \u001b[0;32mIn[43], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epoch_count, scheduler, threshold, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 38\u001b[0m     distance \u001b[39m=\u001b[39m model(x1, x2)\n\u001b[1;32m     39\u001b[0m     loss \u001b[39m=\u001b[39m criterion(distance, y)\n\u001b[1;32m     40\u001b[0m     d \u001b[39m=\u001b[39m distance\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m, in \u001b[0;36mSiameseBasicCNN.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x1, x2):\n\u001b[0;32m---> 14\u001b[0m     output1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresnet(x1)\n\u001b[1;32m     15\u001b[0m     output2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnet(x2)\n\u001b[1;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mpairwise_distance(\n\u001b[1;32m     18\u001b[0m         output1, output2, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torchvision/models/resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m---> 96\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(out)\n\u001b[1;32m     97\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/reidentification/.conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SiameseBasicCNN()\n",
    "lr = 1e-3\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = Adam(model.parameters(), lr)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, results = train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epoch_count=10,\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseBasicCNN(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
